#!/bin/bash -x
#SBATCH -J CoPtCo_Npl5_ncp6
#SBATCH --nodes=64
#SBATCH --ntasks=768
#SBATCH --ntasks-per-node=12
#SBATCH --cpus-per-task=4
#SBATCH -e error-%j.err
#SBATCH -o output-%j.out
#SBATCH --mail-user=f.guimaraes@fz-juelich.de
#SBATCH --mail-type=ALL
##SBATCH -p large
#SBATCH --time=3:30:00

ulimit -s hard
ulimit -n hard

### start of jobscript
# OMP environment variable:
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
export MKL_NUM_THREADS=1
# correct stack size to avoid segmentation fault error when entering omp parallel region
#export OMP_STACKSIZE=50m

#mpiexec -np 48 --exports=OMP_NUM_THREADS,MKL_NUM_THREADS ./main.exe > saida_omp4_1.out
#perf-report ./main.exe > test_1node_ncp5_perf.out
#scalasca -analyse srun ./main.exe > test_1node_npl5.out
srun ./main_jureca.exe > saida_Npl5_ncp6_magaxis9_${SLURM_NTASKS_PER_NODE}x${SLURM_CPUS_PER_TASK}_3.out

# saida_Npl5_dirE_S_magaxis_L_${SLURM_NTASKS_PER_NODE}x${SLURM_CPUS_PER_TASK}.out

